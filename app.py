import streamlit as st
import re

from qdrant_client import QdrantClient
from qdrant_client.models import Filter, FieldCondition, MatchAny

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_qdrant import QdrantVectorStore
from langchain.prompts import ChatPromptTemplate
from langchain_mistralai import ChatMistralAI
from langchain.schema.output_parser import StrOutputParser

from natasha import Segmenter, NewsEmbedding, NewsMorphTagger, MorphVocab, Doc


if 'search_triggered' not in st.session_state:
    st.session_state.search_triggered = False
if 'page' not in st.session_state:
    st.session_state.page = 1
if "prev_genre" not in st.session_state:
    st.session_state.prev_genre = "–í—Å–µ –∂–∞–Ω—Ä—ã"
    


st.session_state.already_ran = True

with open('./data/stopwords/russian', 'r') as f:
    lines = [line.strip() for line in f.readlines()]
    
stop = set(lines) - {'–Ω–µ', '–Ω–∏'}

segmenter = Segmenter()
emb = NewsEmbedding()
morph_tagger = NewsMorphTagger(emb)
morph_vocab = MorphVocab()


GENRES = [
    '–§—ç–Ω—Ç–µ–∑–∏', '–†–æ–º–∞–Ω', '–§–∞–Ω—Ç–∞—Å—Ç–∏–∫–∞', '–ú–æ–ª–æ–¥–µ–∂–Ω–∞—è –ø—Ä–æ–∑–∞',
    '–ü–æ–ø–∞–¥–∞–Ω—Ü—ã', '–≠—Ä–æ—Ç–∏–∫–∞', '–§–∞–Ω—Ñ–∏–∫', '–î–µ—Ç–µ–∫—Ç–∏–≤—ã',
    '–ü—Ä–æ–∑–∞', '–¢—Ä–∏–ª–ª–µ—Ä—ã', '–ú–∏—Å—Ç–∏–∫–∞/–£–∂–∞—Å—ã', '–†–∞–∑–Ω–æ–µ',
    '–ù–æ–Ω-—Ñ–∏–∫—à–Ω', '–ú–∏–Ω–∏'
]

COLLECTION_NAME = 'books-rec-project3'

model_name = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'
model_kwargs = {'device': 'cpu'}
encode_kwargs = {'normalize_embeddings': True}

hf = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
)

API_TOKEN = ''

llm = ChatMistralAI(
    model = 'mistral-small',
    temperature = .7,
    max_tokens=2000,
    api_key=API_TOKEN
)

rag_prompt = ChatPromptTemplate.from_messages([
    ("system", """–¢—ã —ç–∫—Å–ø–µ—Ä—Ç-–∞–Ω–∞–ª–∏—Ç–∏–∫ –∫–Ω–∏–≥–∞–º —Å –º–Ω–æ–≥–æ–ª–µ—Ç–Ω–∏–º –æ–ø—ã—Ç–æ–º –∏ –æ—Ç–ª–∏—á–Ω—ã–º —á—É–≤—Å—Ç–≤–æ–º —é–º–æ—Ä–∞! üéØ
    –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ –∫–Ω–∏–≥–µ –∏ –ø—Ä–µ–¥—É–≥–∞–¥–∞—Ç—å —Å—é–∂–µ—Ç.

    –°—Ç–∏–ª—å –∞–Ω–∞–ª–∏–∑–∞:
    - –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –æ—Ç–≤–µ—Ç —Å —ç–º–æ–¥–∑–∏ –∏ –∑–∞–±–∞–≤–Ω—ã–º–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏
    - –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, —ç—Ç–æ –≤–∞–∂–Ω–æ
    - Generate your response in russian language

    –ü–æ–º–Ω–∏: —é–º–æ—Ä –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –¥–æ–±—Ä—ã–º –∏ –Ω–µ –æ—Å–∫–æ—Ä–±–∏—Ç–µ–ª—å–Ω—ã–º. –¶–µ–ª—å - —Å–¥–µ–ª–∞—Ç—å –∞–Ω–∞–ª–∏–∑ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–º!

    –ï—Å–ª–∏ —Å—Ä–µ–¥–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –µ—Å—Ç—å —á—Ç–æ-—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ –∑–∞–±–∞–≤–Ω–æ–µ - –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —ç—Ç–æ –æ—Ç–º–µ—Ç—å! üòÑ"""),

    ("human", """üìä –î–ê–ù–ù–´–ï –î–õ–Ø –≠–ö–°–ü–ï–†–¢–ù–û–ì–û –ú–ù–ï–ù–ò–Ø:
{context}

üéØ –ó–ê–ü–†–û–° –ù–ê –≠–ö–°–ü–ï–†–¢–ò–ó–£: {question}""")
])


def clean_string(text):
    text = text.lower()
    text = re.sub(r'\s{1}[–∞-—è—ë]*\.\.\.$', '', text) #remove bad ending
    text = re.sub(r'<.*?>', ' ', text) # removing html-tags from text
    text = re.sub(r'http\S+|\S+@\S+', ' ', text) # removing links
    text = re.sub(r'[^–∞-—è—ë\s]', ' ', text.lower()) # remove all non-letter symbols
    cleaned_text = re.sub(r'\s+', ' ', text).strip() # remove double or more spaces
    
    return cleaned_text

def lemmantize_words(text):
    lemmas = []
    doc = Doc(text)
    doc.segment(segmenter)
    doc.tag_morph(morph_tagger)
    
    try:
        for token in doc.tokens:
            token.lemmatize(morph_vocab)
            lemmas.append(token.lemma)
                
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞: {text[:30]}... ‚Üí {e}")
        return ''
    
    return ' '.join(lemmas)

def filter_stop_words(text):
    words = []
    for word in text.split():
        if word in stop or len(word) < 2:
            continue
        words.append(word)
        
    return ' '.join(words)

def preprocess_string(string):
    if not isinstance(string, str) or len(string.strip()) == 0: # fool-protection
        return ''
    
    string = clean_string(string)
    string = lemmantize_words(string)
    string = filter_stop_words(string)
    
    return string


def render_navigation(num_pages, location):
    col1, col2, col3 = st.columns([1,2,1])
    with col1:
        if st.button("‚¨ÖÔ∏è –ù–∞–∑–∞–¥", key=f"prev_{location}"):
            if st.session_state.page > 1:
                st.session_state.page -=1
    with col2:
        st.markdown(f"**–°—Ç—Ä–∞–Ω–∏—Ü–∞ {st.session_state.page} –∏–∑ {num_pages}**")
    with col3:
        if st.button("–í–ø–µ—Ä—ë–¥ ‚û°Ô∏è", key=f"next_{location}"):
            if st.session_state.page < num_pages:
                st.session_state.page += 1

def search_books(query, genre, top_k=5):
    client = QdrantClient(path='./db/qdrant_db')

    vector_store = QdrantVectorStore(
        client=client,
        collection_name=COLLECTION_NAME,
        embedding=hf
    )
    
    my_filter = Filter()
    
    if genre in GENRES:
        my_filter = Filter(
            must = [
                FieldCondition(
                    key='metadata.main_genre',
                    match=MatchAny(any=[genre])
                )
            ]
        )
    
    results = vector_store.similarity_search(
        preprocess_string(query),
        filter=my_filter,
        k=top_k
    )
    
    client.close()

    return results

def format_book(book):
    formatted_book = f"""
    –ê–≤—Ç–æ—Ä—ã: {book.get('author', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}
    –ù–∞–∑–≤–∞–Ω–∏–µ: {book.get('title', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}
    –ö–∞—Ä—Ç–∏–Ω–∫–∞: {book.get('img_url', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}
    –°—ã–ª–∫–∞: {book.get('book_url', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}

    –ö—Ä–∞—Ç–∫–æ–µ –û–ø–∏—Å–∞–Ω–∏–µ: {book.get('annotation', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}...
    """

    return formatted_book

def generate_text(doc):
    query = '–ß—Ç–æ —Ç—ã –¥—É–º–∞–µ—à—å –ø—Ä–æ —ç—Ç—É –∫–Ω–∏–≥—É, –µ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∏ –µ–µ –∞–≤—Ç–æ—Ä–∞?'
    formatted_context = format_book(doc.metadata)
    input_data = {
        "context": formatted_context,
        "question": query
    }
    formatted_prompt = rag_prompt.format(**input_data)
    
    # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç
    response = llm.invoke(formatted_prompt)
    final_answer = StrOutputParser().parse(response)

    return final_answer.content

def reset_generated_texts():
    for k in list(st.session_state.keys()):
        if k.startswith("generated_"):
            del st.session_state[k]


st.title('üìö –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ü–æ–∏—Å–∫ –ö–Ω–∏–≥')

col1, col2 = st.columns([4,1])
with col1:
    query = st.text_input(
        '–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å', label_visibility='collapsed'
    )
with col2:
    if st.button('–ù–∞–π—Ç–∏'):
        reset_generated_texts() 
        st.session_state.search_triggered = True
        st.session_state.page = 1
    
genre = st.selectbox("üìö –§–∏–ª—å—Ç—Ä –ø–æ –∂–∞–Ω—Ä—É", ['–í—Å–µ –∂–∞–Ω—Ä—ã'] + GENRES)

if genre != st.session_state.prev_genre:
    reset_generated_texts() 
    st.session_state.page = 1
    st.session_state.prev_genre = genre
    
    
items_per_page = st.slider(
    '–ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –∫–Ω–∏–≥', min_value=1, max_value=10, value=5, step=1
)

if st.session_state.search_triggered:
    if query != '':
        results = search_books(query, genre, items_per_page)
    else:
        results = []
    
    st.markdown(f"üîé **–ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:** {len(results)}")
    
    for idx, doc in enumerate(results):
        with st.container():
            book_col1, book_col2 = st.columns([1,4])
            with book_col1:
                st.image(doc.metadata.get('img_url', 'https://img.freepik.com/premium-vector/broken-image-icon_268104-8936.jpg'))
            with book_col2:
                st.markdown(f'**{doc.metadata.get('author', 'No author')}**')
                st.markdown(
                    f'[**{doc.metadata.get('title','No title')}**]({doc.metadata.get('book_url','No book_url')})'
                )
                st.markdown(f'{doc.metadata.get('annotation','No annotation')}')
                st.markdown(f'üìñ _–ñ–∞–Ω—Ä: {doc.metadata.get('main_genre','No main_genre')}_')
                
                btn_key = f"generate_btn_{idx}"
                if st.button("ü™Ñ –≠–∫—Å–ø–µ—Ä—Ç–Ω–æ–µ –º–Ω–µ–Ω–∏–µ", key=btn_key):
                    st.session_state[f"generated_{idx}"] = generate_text(doc)

                # === Show generated text if exists ===
                generated_key = f"generated_{idx}"
                if generated_key in st.session_state:
                    st.markdown(f'**–û—Ç–≤–µ—Ç –≠–∫—Å–ø–µ—Ä—Ç–∞:**\n\n{st.session_state[generated_key]}')
            st.markdown('---')
            